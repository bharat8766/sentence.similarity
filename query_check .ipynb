{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensroflow hub module for Universal sentence Encoder\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
    "#@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \n",
    "#\"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "start = time.time()\n",
    "embed = hub.Module(module_url)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path  = \"path/to/text_file/\"\n",
    "with open(path+\"text_filename\",'r') as f:\n",
    "    text  = f.read()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = input_filename+'.txt'\n",
    "# with open(filename, 'r') as data:\n",
    "#   tr = data.read()\n",
    "# tr = tr.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file1 = open(\"results/query_results/66c.txt\",\"w\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitParagraphIntoSentences(paragraph):\n",
    "    ''' break a paragraph into sentences\n",
    "        and return a list '''\n",
    "    import re\n",
    "    # to split by multile characters\n",
    "\n",
    "    #   regular expressions are easiest (and fastest)\n",
    "    sentenceEnders = re.compile('[!.?]')\n",
    "    sentenceList = sentenceEnders.split(paragraph)\n",
    "    return sentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    p = tr\n",
    "    nlst = []\n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "    sentences = splitParagraphIntoSentences(p)\n",
    "    i = 0\n",
    "    for s in sentences:\n",
    "        \n",
    "        #print(i)\n",
    "        k = str(i) + \") \" + s.strip()\n",
    "        i = i+1\n",
    "        #nlst.append[k]\n",
    "#         table  = k\n",
    "#         display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "        file1.write(\"\\n\") \n",
    "            \n",
    "        file1.writelines(k) \n",
    "        file1.write(\"\\n\")\n",
    "        print (k)\n",
    "#       \n",
    "#     s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for text embeddings : \n",
    "def get_features(texts):\n",
    "    if type(texts) is str:\n",
    "        texts = [texts]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        return sess.run(embed(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing the text via removing stop words and some other symbols\n",
    "def remove_stopwords(stop_words, tokens):\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        if not token in stop_words:\n",
    "            res.append(token)\n",
    "    return res\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.encode('ascii', errors='ignore').decode()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'#+', ' ', text )\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n",
    "    #text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"won't\", \"will not \", text)\n",
    "    text = re.sub(r\"isn't\", \"is not \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, 'v')\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "        lemma_list.append(lemma)\n",
    "    # return [ lemmatizer.lemmatize(token, 'v') for token in tokens ]\n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "def process_all(text):\n",
    "    text = process_text(text)\n",
    "    return ' '.join(remove_stopwords(stop_words, text.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sentences\n",
    "data_processed = list(map(process_text, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating embedding vectors of the data\n",
    "BASE_VECTORS = get_features(data)\n",
    "BASE_VECTORS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cosine similarity metric for finding the semantic similarity between the splitted sentences and the phrase/query \n",
    "# that is needed to be checked.\n",
    "def cosine_similarity(v1, v2):\n",
    "    mag1 = np.linalg.norm(v1)\n",
    "    mag2 = np.linalg.norm(v2)\n",
    "    if (not mag1) or (not mag2):\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (mag1 * mag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_similarity(text1, text2):\n",
    "    vec1 = get_features(text1)[0]\n",
    "    vec2 = get_features(text2)[0]\n",
    "    print(vec1.shape)\n",
    "    return cosine_similarity(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_search(query, data, vectors):\n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "    \n",
    "    query = process_text(query)\n",
    "    print(\"Extracting features...\")\n",
    "    query_vec = get_features(query)[0].ravel()\n",
    "    res = []\n",
    "    for i, d in enumerate(data):\n",
    "        qvec = vectors[i].ravel()\n",
    "        sim = cosine_similarity(query_vec, qvec)*100\n",
    "        res.append((sim, d[:100], i))\n",
    "    sr  = sorted(res, key=lambda x : x[0], reverse=True)\n",
    "# appending the results in the text file\n",
    "    file1.write(\"\\n\") \n",
    "    file1.write(\"Querry Result :  \\n\")\n",
    "    for i in sr:        \n",
    "        file1.writelines(str(i)) \n",
    "        file1.write(\"\\n\")\n",
    "    #file1.close()\n",
    "    table = sr\n",
    "#     print(sr,\"\\n\")\n",
    "#     print()\n",
    "    df = pd.DataFrame(data=np.array(table), columns=['similarity', 'sentence', 'sent_index'])\n",
    "    return df.head()\n",
    "   # return display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = \"sentence to be checked in the paragraph/text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.write(\"\\n\") \n",
    "file1.write(query1)\n",
    "print(\"Query : \",query1,\"\\n\")\n",
    "semantic_search(query1, data_processed, BASE_VECTORS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
